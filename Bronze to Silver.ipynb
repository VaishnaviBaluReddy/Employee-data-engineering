{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "844774ba-37cc-4ed6-906c-ab694ecd7b8f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./EDP_Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4051ae12-fbc5-4957-9de3-52ab41951003",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def tblExists(tbl):\n",
    "    try:\n",
    "        qry=f\"SELECT * FROM {tbl}\"\n",
    "        spark.sql(qry)\n",
    "    except:\n",
    "        return False;\n",
    "    else: \n",
    "        return True;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "867cd336-2620-4d27-bf68-ba7bfde24fc2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def load_table(df_cfg):\n",
    "    try:\n",
    "        tbl_nm = df_cfg['entity_nm']\n",
    "        tbl_brnz_nm = f'edp_dev.{tbl_nm}_brnz'\n",
    "        table_slv = f'edp_dev.{tbl_nm}_slv'\n",
    "        SourceColumnKey = df_cfg['src_clm_key']\n",
    "        rule_qry = df_cfg['rule_qry']\n",
    "        upperbound = df_cfg['wtr_mrk_clm_val']\n",
    "        LdSrtTimenrws = datetime.now().isoformat()\n",
    "\n",
    "        # Fetching the watermark\n",
    "        wtrmrkupqry = f\"\"\"SELECT wtr_mrk_clm_val FROM edp_dev.incremental_load_cfg WHERE entity_nm='{tbl_nm}'\"\"\"\n",
    "        wtrmrk = spark.sql(wtrmrkupqry).collect()[0][0]\n",
    "\n",
    "        # Fetching bronze data\n",
    "        df_brnz = spark.sql(f\"\"\"SELECT * FROM {tbl_brnz_nm}\"\"\")\n",
    "        initial_row_count = df_brnz.count()\n",
    "        if df_brnz.isEmpty():\n",
    "            MsgDescnorws=f'No rows exists for the table : {table_slv}'\n",
    "            WorkerCount=worker_count\n",
    "            LdEndTimenrws=datetime.now().isoformat()\n",
    "            log_qry_no_rws = f\"\"\"INSERT INTO EDP_DEV.process_inventory_log (src_nm, entity_nm, pip_nm, run_id, src_rec_cnt, dl_cpy_status,dl_rec_cnt, dl_fs_nm, dl_dr_nm,dl_fl_nm, ing_strt_time, ing_end_time, pip_type, zn_info, wtr_mrk_cur_val, ld_dt_time)VALUES (null, '{tbl_nm}', null,null, 0, 'Failure', 0,null,null,null,'{LdSrtTimenrws}','{LdEndTimenrws}',null,'slv','{wtrmrk}', '{LdEndTimenrws}')\"\"\"\n",
    "            spark.sql(log_qry_no_rws)\n",
    "\n",
    "        if not tblExists(table_slv):\n",
    "            df_brnz = spark.sql(rule_qry)\n",
    "            df_brnz.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"True\").saveAsTable(table_slv)\n",
    "            cleaned_row_count =  df_brnz.count()\n",
    "            ExecutionStatus='Success'\n",
    "            LdEndTimenrws=datetime.now().isoformat()\n",
    "            DataRead=df_brnz.count()\n",
    "            RowsCopied=DataRead\n",
    "                ##Log message in the log table\n",
    "            log_qry_no_rws = f\"\"\"INSERT INTO EDP_DEV.process_inventory_log (src_nm, entity_nm, pip_nm, run_id, src_rec_cnt, dl_cpy_status,dl_rec_cnt, dl_fs_nm, dl_dr_nm,dl_fl_nm, ing_strt_time, ing_end_time, pip_type, zn_info, wtr_mrk_cur_val, ld_dt_time)VALUES (null, '{tbl_nm}', null,null, {DataRead},'{ExecutionStatus}', {RowsCopied},null,null,null,'{LdSrtTimenrws}','{LdEndTimenrws}',null,'slv','{wtrmrk}', '{LdEndTimenrws}')\"\"\"\n",
    "            spark.sql(log_qry_no_rws)\n",
    "\n",
    "            err_rw_cnt = initial_row_count - cleaned_row_count\n",
    "            log_rule_execution = f\"\"\"INSERT INTO EDP_DEV.data_quality_rule_execution_log (lob_nm,src_nm,entity_nm,rule_nm,rule_qry_exec,err_rw_cnt,exec_lg_dt_time) VALUES (null, null, '{tbl_nm}', null, null, '{err_rw_cnt}', '{LdEndTimenrws}')\"\"\"\n",
    "        else:\n",
    "            # Perform incremental load\n",
    "            wtrmrkbrzqry = f'SELECT MAX(watermark) AS watermark FROM {tbl_brnz_nm}'\n",
    "            wtrmrkbrz = spark.sql(wtrmrkbrzqry).first()['watermark']\n",
    "            print(\"wtrmrkbrz\", wtrmrkbrz)\n",
    "            wtrmrkslvqry = f'SELECT MAX(watermark) AS watermark FROM {table_slv}'\n",
    "            wtrmrkslv = spark.sql(wtrmrkslvqry).first()['watermark']\n",
    "            print(\"wtrmrkslv\", wtrmrkslv)\n",
    "            data_qry = f\"\"\"SELECT * FROM {tbl_brnz_nm} WHERE watermark > '{wtrmrkslv}' AND watermark <= '{wtrmrkbrz}'\"\"\"\n",
    "\n",
    "            slv_df = spark.sql(data_qry)\n",
    "            initial_row_count = slv_df.count()\n",
    "            \n",
    "            if slv_df.isEmpty():\n",
    "                LdEndTimenrws=datetime.now().isoformat()\n",
    "                log_qry_no_rws = f\"\"\"INSERT INTO EDP_DEV.process_inventory_log (src_nm, entity_nm, pip_nm, run_id, src_rec_cnt, dl_cpy_status,dl_rec_cnt, dl_fs_nm, dl_dr_nm,dl_fl_nm, ing_strt_time, ing_end_time, pip_type, zn_info, wtr_mrk_cur_val, ld_dt_time)VALUES (null, '{tbl_nm}', null,null, 0, 'Failure', 0,null,null,null,'{LdSrtTimenrws}','{LdEndTimenrws}',null,'slv','{wtrmrk}', '{LdEndTimenrws}')\"\"\"\n",
    "                spark.sql(log_qry_no_rws)\n",
    "\n",
    "            slv_df = spark.sql(rule_qry)\n",
    "            cleaned_row_count = slv_df.count()\n",
    "            window_spec = Window.partitionBy(SourceColumnKey).orderBy(col(\"watermark\").desc())\n",
    "            slv_df = slv_df.withColumn(\"row_num\", row_number().over(window_spec)).filter(col(\"row_num\") == 1).drop(\"row_num\")\n",
    "            print(\"cleaned slv_df\")\n",
    "\n",
    "            slvdeltatbl = DeltaTable.forName(spark, table_slv)\n",
    "            slvnm = 'cpath'\n",
    "            brznm = 'dpath'\n",
    "            condtn = f\"{slvnm}.{SourceColumnKey} = {brznm}.{SourceColumnKey}\"\n",
    "\n",
    "            slvdeltatbl = DeltaTable.forName(spark, table_slv)\n",
    "            slvnm = 'cpath'\n",
    "            brznm = 'dpath'\n",
    "            condtn = f\"{slvnm}.{SourceColumnKey} = {brznm}.{SourceColumnKey}\"\n",
    "\n",
    "            slvdeltatbl.alias(slvnm).merge(\n",
    "                slv_df.alias(brznm), condtn\n",
    "            ).whenMatchedUpdate(\n",
    "                condition=f\"{brznm}.watermark > {slvnm}.watermark\",\n",
    "                set={\n",
    "                    \"Emp_Id\": col(f\"{brznm}.Emp_Id\"),\n",
    "                    \"Emp_nm\": col(f\"{brznm}.Emp_nm\"),\n",
    "                    \"Dept_Id\": col(f\"{brznm}.Dept_Id\"),\n",
    "                    \"createdt\": col(f\"{brznm}.createdt\"),\n",
    "                    \"modifieddt\": col(f\"{brznm}.modifieddt\"),\n",
    "                    \"watermark\": greatest(col(f\"{slvnm}.watermark\"), col(f\"{brznm}.watermark\"))\n",
    "                }\n",
    "            ).whenNotMatchedInsert(\n",
    "                values={\n",
    "                    \"Emp_Id\": col(f\"{brznm}.Emp_Id\"),\n",
    "                    \"Emp_nm\": col(f\"{brznm}.Emp_nm\"),\n",
    "                    \"Dept_Id\": col(f\"{brznm}.Dept_Id\"),\n",
    "                    \"createdt\": col(f\"{brznm}.createdt\"),\n",
    "                    \"modifieddt\": col(f\"{brznm}.modifieddt\"),\n",
    "                    \"watermark\": col(f\"{brznm}.watermark\")\n",
    "                }\n",
    "            ).execute()\n",
    "\n",
    "            DataRead= initial_row_count\n",
    "            RowsCopied = cleaned_row_count\n",
    "            LdEndTimenrws=datetime.now().isoformat()\n",
    "            ExecutionStatus='Success'\n",
    "            log_qry_no_rws = f\"\"\"INSERT INTO EDP_DEV.process_inventory_log (src_nm, entity_nm, pip_nm, run_id, src_rec_cnt, dl_cpy_status,dl_rec_cnt, dl_fs_nm, dl_dr_nm,dl_fl_nm, ing_strt_time, ing_end_time, pip_type, zn_info, wtr_mrk_cur_val, ld_dt_time)VALUES (null, '{tbl_nm}', null,null, {DataRead},'{ExecutionStatus}', {RowsCopied},null,null,null,'{LdSrtTimenrws}','{LdEndTimenrws}',null,'slv','{wtrmrk}', '{LdEndTimenrws}')\"\"\"\n",
    "            spark.sql(log_qry_no_rws)\n",
    "\n",
    "            err_rw_cnt = initial_row_count - cleaned_row_count\n",
    "            log_rule_execution = f\"\"\"INSERT INTO EDP_DEV.data_quality_rule_execution_log (lob_nm,src_nm,entity_nm,rule_nm,rule_qry_exec,err_rw_cnt,exec_lg_dt_time) VALUES (null, null, '{tbl_nm}', null, null, '{err_rw_cnt}', '{LdEndTimenrws}')\"\"\"\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "8dd38c16-a393-4195-a1dc-99f1f6726f09",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# qry = f\"\"\"\n",
    "# SELECT ilc.src_nm,ilc.entity_nm,ilc.wtr_mrk_clm_nm,ilc.wtr_mrk_clm_val,ilc.part_clm_nm,ilc.part_clm_val,ilc.interval,ilc.frequency,ilc.clm_dtls,ilc.ref_qry,ilc.src_wtr_mrk_qry,ilc.cfg_upd_wtr_mrk_qry,ilc.batch_sz,ilc.ld_seq,ilc.uc_nm,ilc.sch_nm,ilc.mig_flag,dqr.lob_nm,dqr.rule_nm,dqr.rule_typ,dqr.rule_bs_qry,dqr.rule_qry,dqr.vw_nm,dqr.mig_flag AS dqr_mig_flag\n",
    "# FROM \n",
    "#     edp_dev.incremental_load_cfg ilc\n",
    "# FULL OUTER JOIN\n",
    "#     edp_dev.data_quality_rule dqr\n",
    "# ON \n",
    "#     ilc.entity_nm = dqr.entity_nm\n",
    "# \"\"\"\n",
    "\n",
    "# df_cfg = spark.sql(qry)\n",
    "# dic = df_cfg.collect()\n",
    "# dic = [row.asDict() for row in dic]\n",
    "# print(dic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5d1d742-fa6d-48a8-b822-4f8d40a7da8f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_nm': 'src_emp', 'src_nm': '', 'wtr_mrk_clm_nm': 'modifieddt', 'wtr_mrk_clm_val': '2024-07-01T09:01:12.663955', 'src_clm_key': 'Emp_ID', 'part_clm_nm': '', 'part_clm_val': '', 'interval': 1, 'frequency': 1, 'clm_dtls': '(Select Emp_Id, Emp_nm, Dept_Id, createdt, modifieddt, modifieddt as watermark from EDP_source.src_emp where modifieddt > \"{}\"  and modifieddt <=\"{}\")', 'ref_qry': '', 'src_wtr_mrk_qry': 'select max(\"{}\") from EDP_source.\"{}\"', 'cfg_upd_wtr_mrk_qry': 'update EDP_DEV.incremental_load_cfg set wtr_mrk_clm_val=\"{}\" where entity_nm=\"{}\"', 'batch_sz': '', 'ld_seq': None, 'uc_nm': '', 'sch_nm': '', 'mig_flag': True, 'lob_nm': '', 'rule_nm': 'NULL_CHECK', 'rule_typ': 'Data quality check', 'rule_bs_qry': '', 'rule_qry': 'SELECT * from edp_dev.src_emp_brnz WHERE Emp_Id IS NOT NULL AND Emp_nm IS NOT NULL AND Dept_Id IS NOT NULL AND createdt IS NOT NULL AND modifieddt IS NOT NULL', 'vw_nm': ''}, {'entity_nm': 'src_emp', 'src_nm': '', 'wtr_mrk_clm_nm': 'modifieddt', 'wtr_mrk_clm_val': '2024-07-01T09:01:12.663955', 'src_clm_key': 'Emp_ID', 'part_clm_nm': '', 'part_clm_val': '', 'interval': 1, 'frequency': 1, 'clm_dtls': '(Select Emp_Id, Emp_nm, Dept_Id, createdt, modifieddt, modifieddt as watermark from EDP_source.src_emp where modifieddt > \"{}\"  and modifieddt <=\"{}\")', 'ref_qry': '', 'src_wtr_mrk_qry': 'select max(\"{}\") from EDP_source.\"{}\"', 'cfg_upd_wtr_mrk_qry': 'update EDP_DEV.incremental_load_cfg set wtr_mrk_clm_val=\"{}\" where entity_nm=\"{}\"', 'batch_sz': '', 'ld_seq': None, 'uc_nm': '', 'sch_nm': '', 'mig_flag': True, 'lob_nm': '', 'rule_nm': 'EMPTY_CHECK', 'rule_typ': 'Data quality check', 'rule_bs_qry': '', 'rule_qry': 'SELECT * from edp_dev.src_emp_brnz WHERE Emp_Id != \"\" OR Emp_nm != \"\" OR Dept_Id != \"\" OR createdt != \"\" OR modifieddt != \"\"', 'vw_nm': ''}, {'entity_nm': 'src_emp', 'src_nm': '', 'wtr_mrk_clm_nm': 'modifieddt', 'wtr_mrk_clm_val': '2024-07-01T09:01:12.663955', 'src_clm_key': 'Emp_ID', 'part_clm_nm': '', 'part_clm_val': '', 'interval': 1, 'frequency': 1, 'clm_dtls': '(Select Emp_Id, Emp_nm, Dept_Id, createdt, modifieddt, modifieddt as watermark from EDP_source.src_emp where modifieddt > \"{}\"  and modifieddt <=\"{}\")', 'ref_qry': '', 'src_wtr_mrk_qry': 'select max(\"{}\") from EDP_source.\"{}\"', 'cfg_upd_wtr_mrk_qry': 'update EDP_DEV.incremental_load_cfg set wtr_mrk_clm_val=\"{}\" where entity_nm=\"{}\"', 'batch_sz': '', 'ld_seq': None, 'uc_nm': '', 'sch_nm': '', 'mig_flag': True, 'lob_nm': '', 'rule_nm': 'DUPLICATE_CHECK', 'rule_typ': 'Data quality check', 'rule_bs_qry': '', 'rule_qry': 'SELECT DISTINCT(*) FROM edp_dev.src_emp_brnz', 'vw_nm': ''}]\n"
     ]
    }
   ],
   "source": [
    "qry=f\"\"\"select * from edp_dev.incremental_load_cfg \"\"\"\n",
    "d_qry=f\"\"\"select * from edp_dev.data_quality_rule \"\"\"\n",
    "df_cfg2=spark.sql(d_qry)\n",
    "df_cfg1=spark.sql(qry)\n",
    "df_cfg2.select(col('lob_nm'),col('src_nm'),col('entity_nm'),col('rule_nm'),col('rule_typ'),col('rule_bs_qry'),col('rule_qry'),col('vw_nm'),col('mig_flag'))\n",
    "df_cfg1.select(col('src_nm'),col('entity_nm'),col('wtr_mrk_clm_nm'),col('wtr_mrk_clm_val'),col('part_clm_nm'),col('part_clm_val'),col('interval'),col('frequency'), col('clm_dtls'),col('ref_qry'),col('src_wtr_mrk_qry'),col('cfg_upd_wtr_mrk_qry'),col('batch_sz'),col('ld_seq'),col('uc_nm'),col('sch_nm'),col('mig_flag'))\n",
    "\n",
    "df_cfg = df_cfg1.join(df_cfg2, on=['entity_nm'], how='outer')\n",
    "dic = df_cfg.collect()\n",
    "dic = [row.asDict() for row in dic]\n",
    "print(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13ef8208-359d-43af-8bdb-7c3dc5246301",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "from queue import Queue\n",
    "\n",
    "q = Queue()\n",
    "\n",
    "worker_count = worker_count\n",
    "def run_tasks(function, q):\n",
    "    while not q.empty():\n",
    "        try:\n",
    "            value = q.get()\n",
    "            function(value)\n",
    "        except Exception as e:\n",
    "            output=f'{e}'\n",
    "            print(output)\n",
    "    \n",
    "        finally:\n",
    "            q.task_done()\n",
    "\n",
    "for table in  dic:\n",
    "   q.put(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbdf765c-21e4-4114-b346-00fd483368e4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wtrmrkbrz 2024-07-01T09:01:12.663955\nwtrmrkbrz 2024-07-01T09:01:12.663955\nwtrmrkbrz 2024-07-01T09:01:12.663955\nwtrmrkslv 2024-07-01T09:01:12.663955\nwtrmrkslv 2024-07-01T09:01:12.663955\nwtrmrkslv 2024-07-01T09:01:12.663955\ncleaned slv_df\ncleaned slv_df\ncleaned slv_df\n"
     ]
    }
   ],
   "source": [
    "for i in range(driver_count):\n",
    "    try:\n",
    "        t=Thread(target=run_tasks, args=(load_table, q))\n",
    "        t.daemon = True\n",
    "        t.start()\n",
    "    except Exception as e: \n",
    "        output=f'{e}'\n",
    "        print(output)\n",
    "\n",
    "q.join()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze to silver 19th june",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
