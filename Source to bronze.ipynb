{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "cfee6109-62cd-4396-8f80-c57f71f2b7c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./EDP_Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4dd5a045-2e9f-4234-88f0-47e7a21bf790",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dbutils.fs.rm(\"dbfs:/user/hive/warehouse/edp_dev.db/src_emp_brnz\", True)\n",
    "# qry = f\"DROP TABLE edp_dev.src_emp_brnz\"\n",
    "# spark.sql(qry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99f52347-8d5f-4b95-81a2-e3227adc2d0d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def load_table(df_cfg):\n",
    "    try:\n",
    "        tbl_nm = df_cfg['entity_nm']\n",
    "        destination_table = f'edp_dev.{tbl_nm}_brnz'\n",
    "        wtrmrkclm = df_cfg['wtr_mrk_clm_nm']\n",
    "        wtrmrkqry = df_cfg['src_wtr_mrk_qry']\n",
    "        lowerbound = df_cfg['wtr_mrk_clm_val']\n",
    "        wtrmrkupqry = df_cfg['cfg_upd_wtr_mrk_qry']\n",
    "        src_qry = df_cfg['clm_dtls']\n",
    "        LdSrtTimenrws = datetime.now().isoformat()\n",
    "\n",
    "        # Source watermark\n",
    "        wtrmrkqry = wtrmrkqry.format(wtrmrkclm, tbl_nm).replace('\"', \"\")\n",
    "        wtrmrk = spark.sql(wtrmrkqry).collect()[0][0]\n",
    "        print(tbl_nm)\n",
    "\n",
    "        # Prepare the Source Query for Incremental Load\n",
    "        src_qry = src_qry.format(lowerbound, wtrmrk)\n",
    "        print(src_qry)\n",
    "\n",
    "        # Dataframe to execute the SQL query from configuration table\n",
    "        df = spark.sql(src_qry)\n",
    "        print(df.count())\n",
    "\n",
    "        if df.isEmpty():\n",
    "            # Log message in the log table if no rows returned\n",
    "            MsgDescnorws = f'No incremental rows exists for the table : {tbl_nm}  , with watermark : {lowerbound} in the source Infor LN'\n",
    "            LdEndTimenrws = datetime.now().isoformat()\n",
    "            log_qry_no_rws = f\"\"\"INSERT INTO EDP_DEV.process_inventory_log (src_nm, entity_nm, pip_nm, run_id, src_rec_cnt, dl_cpy_status,dl_rec_cnt, dl_fs_nm, dl_dr_nm,dl_fl_nm, ing_strt_time, ing_end_time, pip_type, zn_info, wtr_mrk_cur_val, ld_dt_time)\n",
    "                                 VALUES (null, '{tbl_nm}', null, null, 0, 'Failure', 0, null, null, null, '{LdSrtTimenrws}', '{LdEndTimenrws}', null, 'brnz', '{wtrmrk}', '{LdEndTimenrws}')\"\"\"\n",
    "            spark.sql(log_qry_no_rws)\n",
    "        else:\n",
    "            # Define the execution log variable\n",
    "            DataRead = df.count()\n",
    "            RowsCopied = DataRead\n",
    "            ExecutionStatus = 'Success'\n",
    "            ZoneInfo = 'bronze'\n",
    "            LdEndTimenrws = datetime.now().isoformat()\n",
    "\n",
    "            # Write the source data in the table with Schema Overwrite\n",
    "            df.write.format(\"delta\").mode(\"append\").option(\"overwriteSchema\", \"True\").saveAsTable(destination_table)\n",
    "            MsgDescription = f'Data loaded successfully for the table : {tbl_nm}  . Last Watermark value fetched from source successfully for the table : {tbl_nm}  . with value: {wtrmrk} '\n",
    "\n",
    "            # Update watermark in the metadata table\n",
    "            wtrmrkupqry = wtrmrkupqry.replace('\"', \"'\").replace('\"', \"'\")\n",
    "            updateqry = wtrmrkupqry.format(wtrmrk, tbl_nm)\n",
    "            spark.sql(updateqry)\n",
    "            MsgDescription += f'Watermark updated successfully in the metadata table : {tbl_nm}  . with value: {wtrmrk} '\n",
    "\n",
    "            # Log message in the log table\n",
    "            log_qry_success = f\"\"\"INSERT INTO EDP_DEV.process_inventory_log (src_nm, entity_nm, pip_nm, run_id, src_rec_cnt, dl_cpy_status,dl_rec_cnt, dl_fs_nm, dl_dr_nm,dl_fl_nm, ing_strt_time, ing_end_time, pip_type, zn_info, wtr_mrk_cur_val, ld_dt_time)\n",
    "                                  VALUES (null, '{tbl_nm}', null, null, {DataRead}, '{ExecutionStatus}', {RowsCopied}, null, null, null, '{LdSrtTimenrws}', '{LdEndTimenrws}', null, 'brnz', '{wtrmrk}', '{LdEndTimenrws}')\"\"\"\n",
    "            spark.sql(log_qry_success)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        tbl_nm=df_cfg['entity_nm']\n",
    "        LdSrtTimenrws=datetime.now().isoformat()\n",
    "        LdEndTimenrws=datetime.now().isoformat()\n",
    "        ## Source watermark\n",
    "        wtrmrkqry=f\"{wtrmrkqry}\".format(wtrmrkclm,tbl_nm)\n",
    "        wtrmrkqry=wtrmrkqry.replace('\"', \"\")\n",
    "        wtrmrkqry=f'{wtrmrkqry}'\n",
    "        wtrmrk=spark.sql(wtrmrkqry).collect()[0][0]\n",
    "\n",
    "        DataRead = 0\n",
    "        RowsCopied=DataRead\n",
    "\n",
    "        tabl_nm = df_cfg['entity_nm']\n",
    "        exec_status = \"Failed\"\n",
    "        zn_info = \"bronze\"\n",
    "        err_cd = None\n",
    "        err_desc = f\"{e}\"\n",
    "        err_desc = err_desc.replace(\"'\", \"\\\\'\")\n",
    "        err_lg_dt_time = tstampiso\n",
    "        \n",
    "        log_qry_failure = f\"\"\"INSERT INTO EDP_DEV.process_inventory_log (src_nm, entity_nm, pip_nm, run_id, src_rec_cnt, dl_cpy_status,dl_rec_cnt, dl_fs_nm, dl_dr_nm,dl_fl_nm, ing_strt_time, ing_end_time, pip_type, zn_info, wtr_mrk_cur_val, ld_dt_time)VALUES (null, '{tbl_nm}', null,null, {DataRead},'{exec_status}', {RowsCopied},null,null,null,'{LdSrtTimenrws}','{LdEndTimenrws}',null,'brnz','{wtrmrk}', '{LdEndTimenrws}')\"\"\"\n",
    "        spark.sql(log_qry_failure)\n",
    "\n",
    "        log_qry_err = f\"\"\"INSERT INTO EDP_DEV.error_log (src_nm, entity_nm, uc_nm, sch_nm, exec_status, zn_info, err_cd, err_desc, err_lg_dt_time, obj_type) VALUES (null, '{tbl_nm}', null, null, '{exec_status}', '{zn_info}', '{err_cd}', '{err_desc}', '{err_lg_dt_time}', null)\"\"\"\n",
    "        spark.sql(log_qry_err)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60d55e91-0648-493b-9858-5daf50213e4c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'src_nm': '', 'entity_nm': 'src_emp', 'wtr_mrk_clm_nm': 'modifieddt', 'wtr_mrk_clm_val': '1900-01-01T13:32:31.893048', 'src_clm_key': 'Emp_ID', 'part_clm_nm': '', 'part_clm_val': '', 'interval': 1, 'frequency': 1, 'clm_dtls': '(Select Emp_Id, Emp_nm, Dept_Id, createdt, modifieddt, modifieddt as watermark from EDP_source.src_emp where modifieddt > \"{}\"  and modifieddt <=\"{}\")', 'ref_qry': '', 'src_wtr_mrk_qry': 'select max(\"{}\") from EDP_source.\"{}\"', 'cfg_upd_wtr_mrk_qry': 'update EDP_DEV.incremental_load_cfg set wtr_mrk_clm_val=\"{}\" where entity_nm=\"{}\"', 'batch_sz': '', 'ld_seq': None, 'uc_nm': '', 'sch_nm': '', 'mig_flag': ''}]\n"
     ]
    }
   ],
   "source": [
    "qry=f\"\"\"select * from edp_dev.incremental_load_cfg \"\"\"\n",
    "df_cfg=spark.sql(qry)\n",
    "df_cfg.select(col('src_nm'),col('entity_nm'),col('wtr_mrk_clm_nm'),col('wtr_mrk_clm_val'),col('part_clm_nm'),col('part_clm_val'),col('interval'),col('frequency'), col('clm_dtls'),col('ref_qry'),col('src_wtr_mrk_qry'),col('cfg_upd_wtr_mrk_qry'),col('batch_sz'),col('ld_seq'),col('uc_nm'),col('sch_nm'),col('mig_flag'))\n",
    "\n",
    "dic = df_cfg.collect()\n",
    "dic = [row.asDict() for row in dic]\n",
    "print(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "796a8ae1-9be1-44d6-99ca-3250692657ab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "from queue import Queue\n",
    "\n",
    "q = Queue()\n",
    "\n",
    "worker_count = worker_count\n",
    "def run_tasks(function, q):\n",
    "    while not q.empty():\n",
    "        try:\n",
    "            value = q.get()\n",
    "            function(value)\n",
    "        except Exception as e:\n",
    "            output=f'{e}'\n",
    "            print(output)\n",
    "    \n",
    "        finally:\n",
    "         q.task_done()\n",
    "\n",
    "for table in  dic:\n",
    "   q.put(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3574d185-dc6a-4b7c-877a-1f3c1cd4b980",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_emp\n(Select Emp_Id, Emp_nm, Dept_Id, createdt, modifieddt, modifieddt as watermark from EDP_source.src_emp where modifieddt > \"1900-01-01T13:32:31.893048\"  and modifieddt <=\"2024-07-02T06:06:48.959820\")\n23\n"
     ]
    }
   ],
   "source": [
    "for i in range(driver_count):\n",
    "    try:\n",
    "        t=Thread(target=run_tasks, args=(load_table, q))\n",
    "        t.daemon = True\n",
    "        t.start()\n",
    "    except Exception as e: \n",
    "        output=f'{e}'\n",
    "        print(output)\n",
    "\n",
    "q.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d06a3fd7-46b0-474a-b22c-171ef2507dd1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3730666354495864,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Source to bronze",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
